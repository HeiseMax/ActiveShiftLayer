{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Sequential, Conv2d, MaxPool2d, Dropout2d, CrossEntropyLoss, ReLU, Linear, Flatten, Module\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if torch.cuda.is_available():\n",
    "    #device = 'cuda'\n",
    "#else:\n",
    "    #device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASL(Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of Sequential(\n",
       "  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (2): ASL()\n",
       "  (3): Dropout2d(p=0.05, inplace=False)\n",
       "  (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Dropout2d(p=0.05, inplace=False)\n",
       "  (7): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (9): Dropout2d(p=0.05, inplace=False)\n",
       "  (10): ReLU()\n",
       "  (11): Flatten(start_dim=1, end_dim=-1)\n",
       "  (12): Linear(in_features=128, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_drop = 0.05\n",
    "\n",
    "NN = Sequential(Conv2d(1, 32, (5, 5)),\n",
    "                MaxPool2d(2,2),\n",
    "                ASL(32),\n",
    "                Dropout2d(p_drop),\n",
    "                Conv2d(32, 64, (5, 5)),\n",
    "                MaxPool2d(2,2),\n",
    "                Dropout2d(p_drop),\n",
    "                Conv2d(64, 128, (2, 2)),\n",
    "                MaxPool2d(2,2),\n",
    "                Dropout2d(p_drop),\n",
    "                ReLU(),\n",
    "                Flatten(),\n",
    "                Linear(128, 10)\n",
    "                )\n",
    "\n",
    "NN.modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(NN.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(NN):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total2 = 0\n",
    "    loss = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = NN(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            total2 += 1\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return loss/total2, 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] train_loss: 0.023\n",
      "(2.3088919878005982, 10.25)\n",
      "[1,   101] train_loss: 2.232\n",
      "(2.115778603553772, 53.38)\n",
      "[1,   201] train_loss: 1.888\n",
      "(1.5047953057289123, 68.36)\n",
      "[1,   301] train_loss: 1.105\n",
      "(0.7493386021256447, 81.19)\n",
      "[1,   401] train_loss: 0.634\n",
      "(0.5087035059928894, 85.93)\n",
      "[1,   501] train_loss: 0.483\n",
      "(0.4115734979510307, 88.32)\n",
      "[2,     1] train_loss: 0.004\n",
      "(0.3572144535556436, 89.48)\n",
      "[2,   101] train_loss: 0.340\n",
      "(0.32170737579464914, 90.45)\n",
      "[2,   201] train_loss: 0.321\n",
      "(0.2905134744942188, 91.15)\n",
      "[2,   301] train_loss: 0.306\n",
      "(0.2700541178137064, 91.89)\n",
      "[2,   401] train_loss: 0.275\n",
      "(0.25317953728139403, 92.3)\n",
      "[2,   501] train_loss: 0.257\n",
      "(0.24296134492382407, 92.38)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = NN(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] train_loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "            print(test_loss(NN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2196a880fa4fcc7858b5f5924d92f43d58ba0f094c316d904dca65e33e09f006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
