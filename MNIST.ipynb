{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from ActiveShiftLayer import ASL\n",
    "from util import test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.version.cuda)\n",
    "#print(torch.zeros(1).cuda())\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "#transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./data/MNIST\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./data/MNIST\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MNIST_Net                                [100, 10]                 --\n",
       "├─Sequential: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [100, 32, 28, 28]         64\n",
       "│    └─CSC_block: 2-2                    [100, 32, 28, 28]         --\n",
       "│    │    └─Sequential: 3-1              --                        2,304\n",
       "│    └─CSC_block: 2-3                    [100, 32, 28, 28]         --\n",
       "│    │    └─Sequential: 3-2              --                        2,304\n",
       "│    └─AvgPool2d: 2-4                    [100, 32, 4, 4]           --\n",
       "│    └─Flatten: 2-5                      [100, 512]                --\n",
       "│    └─Linear: 2-6                       [100, 10]                 5,130\n",
       "==========================================================================================\n",
       "Total params: 9,802\n",
       "Trainable params: 9,802\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 336.72\n",
       "==========================================================================================\n",
       "Input size (MB): 0.31\n",
       "Forward/backward pass size (MB): 220.78\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 221.14\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Models import MNIST_Net\n",
    "\n",
    "p_drop = 0.05\n",
    "input_shape = (batch_size, 1, 28, 28)\n",
    "expansion_rate = 1\n",
    "\n",
    "NN = MNIST_Net(input_shape, 10, expansion_rate, device)\n",
    "\n",
    "summary(NN, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(NN.parameters(), lr=0.05, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] train_loss: 0.715\n",
      "(0.22816641511395575, 92.68)\n",
      "[1,   200] train_loss: 0.192\n",
      "(0.18493165225721897, 93.84)\n",
      "[1,   300] train_loss: 0.165\n",
      "(0.13996522705536335, 95.64)\n",
      "[1,   400] train_loss: 0.147\n",
      "(0.1323805426992476, 95.86)\n",
      "[1,   500] train_loss: 0.113\n",
      "(0.11726483502890915, 96.0)\n",
      "[1,   600] train_loss: 0.107\n",
      "(0.08020057712914422, 97.37)\n",
      "[2,   100] train_loss: 0.103\n",
      "(0.07316928614629432, 97.48)\n",
      "[2,   200] train_loss: 0.093\n",
      "(0.07609701319481246, 97.45)\n",
      "[2,   300] train_loss: 0.100\n",
      "(0.08536210779915564, 97.34)\n",
      "[2,   400] train_loss: 0.096\n",
      "(0.0773756203148514, 97.56)\n",
      "[2,   500] train_loss: 0.085\n",
      "(0.06352138399321121, 97.84)\n",
      "[2,   600] train_loss: 0.090\n",
      "(0.07186561815091409, 97.42)\n",
      "time: 91484375000\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time_ns()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = NN(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] train_loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "            print(test_loss(NN, test_dataloader, criterion, device))\n",
    "    scheduler.step()\n",
    "end = time.process_time_ns()\n",
    "\n",
    "print(f\"time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.484375\n"
     ]
    }
   ],
   "source": [
    "etime = (end - start) * 1e-9\n",
    "print(time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MNIST_conv_Net                           [100, 10]                 --\n",
       "├─Sequential: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [100, 32, 28, 28]         64\n",
       "│    └─BatchNorm2d: 2-2                  [100, 32, 28, 28]         64\n",
       "│    └─ReLU: 2-3                         [100, 32, 28, 28]         --\n",
       "│    └─Conv2d: 2-4                       [100, 32, 28, 28]         9,248\n",
       "│    └─BatchNorm2d: 2-5                  [100, 32, 28, 28]         64\n",
       "│    └─ReLU: 2-6                         [100, 32, 28, 28]         --\n",
       "│    └─Conv2d: 2-7                       [100, 32, 28, 28]         25,632\n",
       "│    └─BatchNorm2d: 2-8                  [100, 32, 28, 28]         64\n",
       "│    └─ReLU: 2-9                         [100, 32, 28, 28]         --\n",
       "│    └─AvgPool2d: 2-10                   [100, 32, 4, 4]           --\n",
       "│    └─Flatten: 2-11                     [100, 512]                --\n",
       "│    └─Linear: 2-12                      [100, 10]                 5,130\n",
       "==========================================================================================\n",
       "Total params: 40,266\n",
       "Trainable params: 40,266\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.74\n",
       "==========================================================================================\n",
       "Input size (MB): 0.31\n",
       "Forward/backward pass size (MB): 120.43\n",
       "Params size (MB): 0.16\n",
       "Estimated Total Size (MB): 120.91\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Models import MNIST_conv_Net\n",
    "\n",
    "p_drop = 0.05\n",
    "input_shape = (batch_size, 1, 28, 28)\n",
    "\n",
    "NN2 = MNIST_conv_Net(input_shape, 10)\n",
    "\n",
    "summary(NN2, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(NN2.parameters(), lr=0.05, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] train_loss: 0.461\n",
      "(0.121927930675447, 96.43)\n",
      "[1,   200] train_loss: 0.131\n",
      "(0.08365229741670191, 97.49)\n",
      "[1,   300] train_loss: 0.107\n",
      "(0.06615080372663215, 97.9)\n",
      "[1,   400] train_loss: 0.083\n",
      "(0.06283143160515464, 98.08)\n",
      "[1,   500] train_loss: 0.081\n",
      "(0.06338795224088244, 97.96)\n",
      "[1,   600] train_loss: 0.074\n",
      "(0.06811735047958792, 97.62)\n",
      "[2,   100] train_loss: 0.065\n",
      "(0.056236699686851355, 98.18)\n",
      "[2,   200] train_loss: 0.056\n",
      "(0.046598938982351686, 98.49)\n",
      "[2,   300] train_loss: 0.058\n",
      "(0.04778041346522514, 98.42)\n",
      "[2,   400] train_loss: 0.060\n",
      "(0.0473111592719215, 98.38)\n",
      "[2,   500] train_loss: 0.058\n",
      "(0.049529263663862365, 98.46)\n",
      "[2,   600] train_loss: 0.057\n",
      "(0.050539112215628845, 98.24)\n",
      "<module 'time' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time_ns()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = NN2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] train_loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "            print(test_loss(NN2, test_dataloader, criterion, device))\n",
    "    scheduler.step()\n",
    "\n",
    "end = time.process_time_ns()\n",
    "\n",
    "ex_time = (end - start) * 1e-9\n",
    "print(time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.25\n"
     ]
    }
   ],
   "source": [
    "print(ex_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MNIST_ownconv_Net' from 'Models' (c:\\Users\\Max Heise\\Documents\\Uni\\Veranstaltungen\\AML\\Abschlussprojekt\\ASL\\Models.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MAXHEI~1\\AppData\\Local\\Temp/ipykernel_14136/2874713656.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mModels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMNIST_ownconv_Net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mp_drop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MNIST_ownconv_Net' from 'Models' (c:\\Users\\Max Heise\\Documents\\Uni\\Veranstaltungen\\AML\\Abschlussprojekt\\ASL\\Models.py)"
     ]
    }
   ],
   "source": [
    "from Models import MNIST_ownconv_Net\n",
    "\n",
    "p_drop = 0.05\n",
    "input_shape = (batch_size, 1, 28, 28)\n",
    "\n",
    "NN7 = MNIST_conv_Net(input_shape, 10)\n",
    "\n",
    "summary(NN7, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(NN7.parameters(), lr=0.05, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time_ns()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = NN7(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] train_loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "            print(test_loss(NN7, test_dataloader, criterion, device))\n",
    "    scheduler.step()\n",
    "\n",
    "end = time.process_time_ns()\n",
    "\n",
    "ex_time = (end - start) * 1e-9\n",
    "print(time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MNIST_Net2                               [100, 10]                 --\n",
       "├─Sequential: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [100, 32, 28, 28]         64\n",
       "│    └─CSC_block: 2-2                    [100, 32, 28, 28]         --\n",
       "│    │    └─Sequential: 3-1              --                        6,720\n",
       "│    └─CSC_block: 2-3                    [100, 32, 28, 28]         --\n",
       "│    │    └─Sequential: 3-2              --                        6,720\n",
       "│    └─CSC_block: 2-4                    [100, 32, 28, 28]         --\n",
       "│    │    └─Sequential: 3-3              --                        6,720\n",
       "│    └─AvgPool2d: 2-5                    [100, 32, 4, 4]           --\n",
       "│    └─Flatten: 2-6                      [100, 512]                --\n",
       "│    └─Linear: 2-7                       [100, 10]                 5,130\n",
       "==========================================================================================\n",
       "Total params: 25,354\n",
       "Trainable params: 25,354\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.48\n",
       "==========================================================================================\n",
       "Input size (MB): 0.31\n",
       "Forward/backward pass size (MB): 682.40\n",
       "Params size (MB): 0.10\n",
       "Estimated Total Size (MB): 682.82\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Models import MNIST_Net2\n",
    "\n",
    "expansion_rate = 3\n",
    "input_shape = (batch_size, 1, 28, 28)\n",
    "\n",
    "NN3 = MNIST_Net2(input_shape, 10)\n",
    "\n",
    "summary(NN3, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(NN3.parameters(), lr=0.05, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] train_loss: 0.509\n",
      "(0.23171740879304706, 93.05)\n",
      "[1,   200] train_loss: 0.176\n",
      "(0.11580315442988649, 96.14)\n",
      "[1,   300] train_loss: 0.130\n",
      "(0.11617182110901922, 96.27)\n",
      "[1,   400] train_loss: 0.132\n",
      "(0.09387944596121088, 96.96)\n",
      "[1,   500] train_loss: 0.134\n",
      "(0.09338684955437201, 96.9)\n",
      "[1,   600] train_loss: 0.102\n",
      "(0.06715060683432966, 97.68)\n",
      "[2,   100] train_loss: 0.091\n",
      "(0.07073068731755484, 97.68)\n",
      "[2,   200] train_loss: 0.087\n",
      "(0.08192401613050607, 97.34)\n",
      "[2,   300] train_loss: 0.084\n",
      "(0.07734284515725448, 97.61)\n",
      "[2,   400] train_loss: 0.082\n",
      "(0.0925876001745928, 97.02)\n",
      "[2,   500] train_loss: 0.088\n",
      "(0.09988075721426867, 96.62)\n",
      "[2,   600] train_loss: 0.080\n",
      "(0.06303482159040869, 97.96)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = NN3(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] train_loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "            print(test_loss(NN3, test_dataloader, criterion, device))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MNIST_Net2                               [100, 10]                 --\n",
       "├─Sequential: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [100, 32, 28, 28]         64\n",
       "│    └─CSC_block: 2-2                    [100, 32, 28, 28]         --\n",
       "│    │    └─Sequential: 3-1              --                        2,304\n",
       "│    └─CSC_block: 2-3                    [100, 32, 28, 28]         --\n",
       "│    │    └─Sequential: 3-2              --                        2,304\n",
       "│    └─CSC_block: 2-4                    [100, 32, 28, 28]         --\n",
       "│    │    └─Sequential: 3-3              --                        2,304\n",
       "│    └─AvgPool2d: 2-5                    [100, 32, 4, 4]           --\n",
       "│    └─Flatten: 2-6                      [100, 512]                --\n",
       "│    └─Linear: 2-7                       [100, 10]                 5,130\n",
       "==========================================================================================\n",
       "Total params: 12,106\n",
       "Trainable params: 12,106\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 502.31\n",
       "==========================================================================================\n",
       "Input size (MB): 0.31\n",
       "Forward/backward pass size (MB): 321.13\n",
       "Params size (MB): 0.05\n",
       "Estimated Total Size (MB): 321.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expansion_rate = 1\n",
    "input_shape = (batch_size, 1, 28, 28)\n",
    "\n",
    "NN4 = MNIST_Net2(input_shape, 10, expansion_rate, device)\n",
    "\n",
    "summary(NN4, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(NN4.parameters(), lr=0.05, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] train_loss: 0.569\n",
      "(0.18625395480543375, 94.0)\n",
      "[1,   200] train_loss: 0.168\n",
      "(0.12762564175296576, 95.99)\n",
      "[1,   300] train_loss: 0.138\n",
      "(0.10866357750957832, 96.47)\n",
      "[1,   400] train_loss: 0.125\n",
      "(0.10119087643921375, 96.86)\n",
      "[1,   500] train_loss: 0.101\n",
      "(0.07436577038373798, 97.6)\n",
      "[1,   600] train_loss: 0.094\n",
      "(0.08836525014252401, 97.2)\n",
      "[2,   100] train_loss: 0.098\n",
      "(0.06228518116287887, 98.07)\n",
      "[2,   200] train_loss: 0.083\n",
      "(0.07463950763689355, 97.66)\n",
      "[2,   300] train_loss: 0.078\n",
      "(0.0749166457017418, 97.52)\n",
      "[2,   400] train_loss: 0.082\n",
      "(0.060831173137994486, 98.1)\n",
      "[2,   500] train_loss: 0.072\n",
      "(0.058170136823318896, 98.15)\n",
      "[2,   600] train_loss: 0.070\n",
      "(0.059499458849313666, 98.13)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = NN4(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] train_loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "            print(test_loss(NN4, test_dataloader, criterion, device))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
